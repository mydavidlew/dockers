Kubernetes for Docker

[[[Install kubectl on Linux]]]
A) Install kubectl binary with curl on Linux
Download the latest release with the command:
$ curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

Validate the binary (optional), download the kubectl checksum file:
$ curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"

Validate the kubectl binary against the checksum file:
$ echo "$(<kubectl.sha256) kubectl" | sha256sum --check

Install kubectl
$ sudo install -o root -g root -m 0755 kubectl /usr/bin/kubectl

Get or describe clusters information
$ kubectl api-resources
$ kubectl get/describe pods -o wide -A
$ kubectl get/describe nodes -o wide -A
$ kubectl get/describe services -o wide -A
$ kubectl get/describe endpoints -o wide -A
$ kubectl get/describe namespaces -o wide -A
$ kubectl get/describe deployments -o wide -A

To restart or reboot all deployment pods
$ kubectl rollout restart deployment <deployment-name>
$ kubectl scale deployment <deployment-name> --replicas=0

To display and edit the current kube proxy configuration
$ kubectl describe -n kube-system cm kube-proxy
$ kubectl get -n kube-system -o yaml cm kube-proxy
$ kubectl edit -n kube-system -o yaml cm kube-proxy

Create and expose a new service deployment, next to allow forwarding firewall to public port
$ kubectl expose deployment <deployment-name> --type=<ClusterIP/NodePort/LoadBalancer/ExternalName> --name=<new-servicename> --port=<ext-port>  --target-port=<int-port>
$ kubectl port-forward svc/<new-servicename> <public-port>:<ext-port>
$ curl http://localhost:<public-port>
$ kubectl delete services <new-servicename>

B) Install using native package management
***Ubuntu, Debian or HypriotOS***
$ sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2 curl
a$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
a$ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
or
b$ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
b$ echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
$ sudo apt-get update
$ sudo apt-get install -y kubectl kubeadm kubelet
$ sudo apt-mark hold kubelet kubeadm kubectl   (*optional)

***CentOS, RHEL or Fedora***
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64       (***)
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch   (***)
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
$ yum install -y kubectl kubeadm kubelet
$ sudo systemctl enable --now kubelet   (*optional)

[[[Optional kubectl configurations]]]
C) Install bash-completion
Bash-completion is provided by many package managers. You can install it with:
$ sudo apt-get install bash-completion
The above commands create "/usr/share/bash-completion/bash_completion", which is the main script of bash-completion. Depending on your package manager, you have to manually source this file in your "~/.bashrc" file.

To find out, reload your shell and run "type _init_completion". If the command succeeds, you're already set, otherwise add the following to your "~/.bashrc" file:
$ source /usr/share/bash-completion/bash_completion
Reload your shell and verify that bash-completion is correctly installed by typing "type _init_completion".

D) Enable kubectl autocompletion 
You now need to ensure that the kubectl completion script gets sourced in all your shell sessions. There are two ways in which you can do this:

- Source the completion script in your "~/.bashrc" file:
$ echo 'source <(kubectl completion bash)' >>~/.bashrc
- Add the completion script to the "/etc/bash_completion.d" directory:
$ kubectl completion bash >/etc/bash_completion.d/kubectl

If you have an alias for kubectl, you can extend shell completion to work with that alias:
$ echo 'alias k=kubectl' >>~/.bashrc
$ echo 'complete -F __start_kubectl k' >>~/.bashrc

[[[Configure cgroup driver used by kubelet on control-plane node]]]
E) Control groups are used to constrain resources that are allocated to processes
When 'systemd' is chosen as the init system for a Linux distribution, the init process generates and consumes a root control group (cgroup) and acts as a cgroup manager. Systemd has a tight integration with cgroups and allocates a cgroup per systemd unit. It's possible to configure your container runtime and the kubelet to use 'cgroupfs'. Using 'cgroupfs' alongside systemd means that there will be two different cgroup managers. A single cgroup manager simplifies the view of what resources are being allocated and will by default have a more consistent view of the available and in-use resources. When there are two cgroup managers on a system, you end up with two views of those resources. In the field, people have reported cases where nodes that are configured to use 'cgroupfs' for the kubelet and Docker, but 'systemd' for the rest of the processes, become unstable under resource pressure. Changing the settings such that your container runtime and kubelet use 'systemd' as the cgroup driver stabilized the system.

Your cgroup driver used by Kubelet must match Docker’s cgroup driver.

$ docker info | grep -i cgroup
you will get a Docker cgroup driver. And with the command

$ sudo cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
the Kubelet configuration. In the column with the flag “–cgroup-driver=” you can see the setting.
--> Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"

When I installed Docker v.1.12 and kubeadm on CentOS 7, the Docker cgroup driver was specified with cgroupfs and for Kubelet with systemd. If you find the same setting, you can adjust the kubelet config with the following command:
$ sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
or
$ CG=$(sudo docker info 2>/dev/null | sed -n 's/Cgroup Driver: \(.*\)/\1/p')
$ sed -i "s/cgroup-driver=systemd/cgroup-driver=$CG/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

If an adjustment has been made, your Kubelet must be restarted.
$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet

[[[Running KIND Inside A Kubernetes Cluster]]]
F) KinD: Kubernetes in Docker like Docker in Docker, but for Kubernetes!
KinD or Kubernetes in Docker is a suite of tools for local Kubernetes “clusters” where each “node” is a Docker container. KinD was primarily designed for testing Kubernetes itself, but may be used for local development or CI. kind is divided into go packages implementing most of the functionality, a command line for users, and a “node” base image. The intent is that the kind the suite of packages should eventually be importable and reusable by other tools (e.g. kubetest) while the CLI provides a quick way to use and debug these packages.
https://kind.sigs.k8s.io/docs/user/quick-start/#installation
https://github.com/kubernetes-sigs/kind/releases

$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.22.0/kind-linux-amd64
$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64
$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.11.1/kind-linux-amd64
$ sudo install -o root -g root -m 0755 kind /usr/bin/kind
$ kind version

Once you have docker running you can create a cluster with:
$ kind create cluster

To delete your cluster use:
$ kind delete cluster

To create a cluster from Kubernetes source:
- ensure that Kubernetes is cloned in "$(go env GOPATH)/src/k8s.io/kubernetes"
- build a node image and create a cluster with:
$ kind build node-image
$ kind create cluster --image kindest/node:latest

[[[MetalLB for Kubernetes]]]
G) Reference: https://metallb.universe.tf/ “MetalLB is a load-balancer implementation for bare metal Kubernetes clusters, using standard routing protocols.”
MetalLB hooks into your Kubernetes cluster, and provides a network load-balancer implementation. In short, it allows you to create Kubernetes services of type “LoadBalancer” in clusters that don’t run on a cloud provider, and thus cannot simply hook into paid products to provide load-balancers. Once MetalLB has assigned an external IP address to a service, it needs to make the network beyond the cluster aware that the IP “lives” in the cluster. MetalLB uses standard routing protocols to achieve this: ARP, NDP, or BGP.

If you’re using kube-proxy in IPVS mode, since Kubernetes v1.14.2 you have to enable strict ARP mode. Note, you don’t need this if you’re using kube-router as service-proxy because it is enabling strict arp by default.
You can achieve this by editing kube-proxy config in current cluster:
$ kubectl edit configmap -n kube-system kube-proxy
and set:
>> apiVersion: kubeproxy.config.k8s.io/v1alpha1
>> kind: KubeProxyConfiguration
>> mode: "ipvs"
>> ipvs:
>>   strictARP: true
You can also add this configuration snippet to your kubeadm-config, just append it with '---' after the main configuration.

If you are trying to automate this change, these shell snippets may help you:
-> see what changes would be made, returns nonzero returncode if different
$ kubectl get configmap kube-proxy -n kube-system -o yaml | sed -e "s/strictARP: false/strictARP: true/" | kubectl diff -f - -n kube-system

-> actually apply the changes, returns nonzero returncode on errors only
$ kubectl get configmap kube-proxy -n kube-system -o yaml | sed -e "s/strictARP: false/strictARP: true/" | kubectl apply -f - -n kube-system

To install MetalLB, apply the manifest:
$>kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/master/manifests/namespace.yaml
$>kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/master/manifests/metallb.yaml
$>#ubectl apply -f https://kind.sigs.k8s.io/examples/loadbalancer/metallb-configmap.yaml
$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml
$ kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml
On first install only
$ kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"

This will deploy MetalLB to your cluster, under the 'metallb-system' namespace. The components in the manifest are:
-> The 'metallb-system/controller' deployment. This is the cluster-wide controller that handles IP address assignments.
-> The 'metallb-system/speaker' daemonset. This is the component that speaks the protocol(s) of your choice to make the services reachable.
-> Service accounts for the controller and speaker, along with the RBAC permissions that the components need to function.
The installation manifest does not include a configuration file. MetalLB’s components will still start, but will remain idle until you define and deploy a configmap. The 'memberlist' secret contains the 'secretkey' to encrypt the communication between speakers for the fast dead node detection.

Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. Traffic routing is controlled by rules defined on the Ingress resource. An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, terminate SSL / TLS, and offer name-based virtual hosting.
An Ingress controller is responsible for fulfilling the Ingress, usually with a load balancer, though it may also configure your edge router or additional frontends to help handle the traffic. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type Service.Type=NodePort or Service.Type=LoadBalancer.
-> https://kubernetes.io/docs/concepts/services-networking/ingress/
-> https://kubernetes.github.io/ingress-nginx/deploy/#bare-metal

Bare-metal ingress controller installation, run the following command:
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.45.0/deploy/static/provider/baremetal/deploy.yaml

To check if the ingress controller pods have started, run the following command:
$ kubectl get pods -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx --watch

[[[Deploying the Dashboard UI]]]
H) Install the Web UI (Dashboard)
Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.

The Dashboard UI is not deployed by default. To deploy it, run the following command:
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml

Command line proxy - You can access Dashboard using the kubectl command-line tool by running the following command:
$ kubectl proxy

Creating a dashboard admin user
In this guide, we will find out how to create a new user using Service Account mechanism of Kubernetes, grant this user admin permissions and login to Dashboard using bearer token tied to this user.
IMPORTANT: Make sure that you know what you are doing before proceeding. Granting admin privileges to Dashboard's Service Account might be a security risk.
For each of the following snippets for "ServiceAccount" and "ClusterRoleBinding", you should copy them to new manifest files like "dashboard-adminuser.yaml" and use "kubectl apply -f dashboard-adminuser.yaml" to create them.

Creating a Service Account
We are creating Service Account with name "admin-user" in namespace "kubernetes-dashboard" first.
$ cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
EOF

Creating a ClusterRoleBinding
In most cases after provisioning cluster using "kops", "kubeadm" or any other popular tool, the "ClusterRole" "cluster-admin" already exists in the cluster. We can use it and create only "ClusterRoleBinding" for our "ServiceAccount". If it does not exist then you need to create this role first and grant required privileges manually.
$ cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

Getting a Bearer Token - Now we need to find token we can use to log in. Execute following command:
$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"

Accessing the Dashboard UI - To protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default. Currently, Dashboard only supports logging in with a Bearer Token. Now copy the token and paste it into Enter token field on the login screen at below url. Click "Sign in" button and that's it. You are now logged in as an admin.
Kubectl will make Dashboard available at:
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Clean up dashboard admin user and next steps
Remove the admin "ServiceAccount" and "ClusterRoleBinding".
$ kubectl -n kubernetes-dashboard delete serviceaccount admin-user
$ kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user

Notes:
Port configurations for Kubernetes Services
In Kubernetes there are several different port configurations for Kubernetes services:
1) Port - (external port to map) exposes the Kubernetes service on the specified port within the cluster. Other pods within the cluster can communicate with this server on the specified port.
2) TargetPort - (internal container port) is the port on which the service will send requests to, that your pod will be listening on. Your application in the container will need to be listening on this port also.
3) NodePort - (public port combine with external IP) exposes a service externally to the cluster by means of the target nodes IP address and the NodePort. NodePort is the default setting if the port field is not specified.
