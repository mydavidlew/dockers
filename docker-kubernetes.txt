Kubernetes for Docker

[[[Install kubectl on Linux]]]
A) Install kubectl binary with curl on Linux
Download the latest release with the command:
$ curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

Validate the binary (optional), download the kubectl checksum file:
$ curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"

Validate the kubectl binary against the checksum file:
$ echo "$(<kubectl.sha256) kubectl" | sha256sum --check

Install kubectl
$ sudo install -o root -g root -m 0755 kubectl /usr/bin/kubectl

Get clusters information
$ kubectl api-resources
$ kubectl describe pods
$ kubectl describe services
$ kubectl describe endpoints
$ kubectl describe namespaces
$ kubectl describe deployments

B) Install using native package management
***Ubuntu, Debian or HypriotOS***
$ sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2 curl
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
$ sudo apt-get update
$ sudo apt-get install -y kubectl kubeadm kubelet
$ sudo apt-mark hold kubelet kubeadm kubectl   (*optional)

***CentOS, RHEL or Fedora***
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64       (***)
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch   (***)
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
$ yum install -y kubectl kubeadm kubelet
$ sudo systemctl enable --now kubelet   (*optional)

[[[Optional kubectl configurations]]]
C) Install bash-completion
Bash-completion is provided by many package managers. You can install it with:
$ sudo apt-get install bash-completion
The above commands create "/usr/share/bash-completion/bash_completion", which is the main script of bash-completion. Depending on your package manager, you have to manually source this file in your "~/.bashrc" file.

To find out, reload your shell and run "type _init_completion". If the command succeeds, you're already set, otherwise add the following to your "~/.bashrc" file:
$ source /usr/share/bash-completion/bash_completion
Reload your shell and verify that bash-completion is correctly installed by typing "type _init_completion".

D) Enable kubectl autocompletion 
You now need to ensure that the kubectl completion script gets sourced in all your shell sessions. There are two ways in which you can do this:

- Source the completion script in your "~/.bashrc" file:
$ echo 'source <(kubectl completion bash)' >>~/.bashrc
- Add the completion script to the "/etc/bash_completion.d" directory:
$ kubectl completion bash >/etc/bash_completion.d/kubectl

If you have an alias for kubectl, you can extend shell completion to work with that alias:
$ echo 'alias k=kubectl' >>~/.bashrc
$ echo 'complete -F __start_kubectl k' >>~/.bashrc

[[[Configure cgroup driver used by kubelet on control-plane node]]]
E) Control groups are used to constrain resources that are allocated to processes
When 'systemd' is chosen as the init system for a Linux distribution, the init process generates and consumes a root control group (cgroup) and acts as a cgroup manager. Systemd has a tight integration with cgroups and allocates a cgroup per systemd unit. It's possible to configure your container runtime and the kubelet to use 'cgroupfs'. Using 'cgroupfs' alongside systemd means that there will be two different cgroup managers. A single cgroup manager simplifies the view of what resources are being allocated and will by default have a more consistent view of the available and in-use resources. When there are two cgroup managers on a system, you end up with two views of those resources. In the field, people have reported cases where nodes that are configured to use 'cgroupfs' for the kubelet and Docker, but 'systemd' for the rest of the processes, become unstable under resource pressure. Changing the settings such that your container runtime and kubelet use 'systemd' as the cgroup driver stabilized the system.

Your cgroup driver used by Kubelet must match Docker’s cgroup driver.

$ docker info | grep -i cgroup
you will get a Docker cgroup driver. And with the command

$ sudo cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
the Kubelet configuration. In the column with the flag “–cgroup-driver=” you can see the setting.
--> Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"

When I installed Docker v.1.12 and kubeadm on CentOS 7, the Docker cgroup driver was specified with cgroupfs and for Kubelet with systemd. If you find the same setting, you can adjust the kubelet config with the following command:
$ sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
or
$ CG=$(sudo docker info 2>/dev/null | sed -n 's/Cgroup Driver: \(.*\)/\1/p')
$ sed -i "s/cgroup-driver=systemd/cgroup-driver=$CG/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

If an adjustment has been made, your Kubelet must be restarted.
$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet

[[[Running KIND Inside A Kubernetes Cluster]]]
F) KinD: Kubernetes in Docker like Docker in Docker, but for Kubernetes!
KinD or Kubernetes in Docker is a suite of tools for local Kubernetes “clusters” where each “node” is a Docker container. KinD was primarily designed for testing Kubernetes itself, but may be used for local development or CI. kind is divided into go packages implementing most of the functionality, a command line for users, and a “node” base image. The intent is that the kind the suite of packages should eventually be importable and reusable by other tools (e.g. kubetest) while the CLI provides a quick way to use and debug these packages.

$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64
$ sudo install -o root -g root -m 0755 kind /usr/bin/kind
$ kind version

Once you have docker running you can create a cluster with:
$ kind create cluster

To delete your cluster use:
$ kind delete cluster

To create a cluster from Kubernetes source:
- ensure that Kubernetes is cloned in "$(go env GOPATH)/src/k8s.io/kubernetes"
- build a node image and create a cluster with:
$ kind build node-image
$ kind create cluster --image kindest/node:latest

[[[Deploying the Dashboard UI]]]
G) Install the Web UI (Dashboard)
Dashboard is a web-based Kubernetes user interface. You can use Dashboard to deploy containerized applications to a Kubernetes cluster, troubleshoot your containerized application, and manage the cluster resources. You can use Dashboard to get an overview of applications running on your cluster, as well as for creating or modifying individual Kubernetes resources (such as Deployments, Jobs, DaemonSets, etc). For example, you can scale a Deployment, initiate a rolling update, restart a pod or deploy new applications using a deploy wizard. Dashboard also provides information on the state of Kubernetes resources in your cluster and on any errors that may have occurred.

The Dashboard UI is not deployed by default. To deploy it, run the following command:
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml

Command line proxy - You can access Dashboard using the kubectl command-line tool by running the following command:
$ kubectl proxy

Creating a dashboard admin user
In this guide, we will find out how to create a new user using Service Account mechanism of Kubernetes, grant this user admin permissions and login to Dashboard using bearer token tied to this user.
IMPORTANT: Make sure that you know what you are doing before proceeding. Granting admin privileges to Dashboard's Service Account might be a security risk.
For each of the following snippets for "ServiceAccount" and "ClusterRoleBinding", you should copy them to new manifest files like "dashboard-adminuser.yaml" and use "kubectl apply -f dashboard-adminuser.yaml" to create them.

Creating a Service Account
We are creating Service Account with name "admin-user" in namespace "kubernetes-dashboard" first.
$ cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
EOF

Creating a ClusterRoleBinding
In most cases after provisioning cluster using "kops", "kubeadm" or any other popular tool, the "ClusterRole" "cluster-admin" already exists in the cluster. We can use it and create only "ClusterRoleBinding" for our "ServiceAccount". If it does not exist then you need to create this role first and grant required privileges manually.
$ cat <<EOF | kubectl apply -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
EOF

Getting a Bearer Token - Now we need to find token we can use to log in. Execute following command:
$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath="{.secrets[0].name}") -o go-template="{{.data.token | base64decode}}"

Accessing the Dashboard UI - To protect your cluster data, Dashboard deploys with a minimal RBAC configuration by default. Currently, Dashboard only supports logging in with a Bearer Token. Now copy the token and paste it into Enter token field on the login screen at below url. Click "Sign in" button and that's it. You are now logged in as an admin.
Kubectl will make Dashboard available at:
http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Clean up dashboard admin user and next steps
Remove the admin "ServiceAccount" and "ClusterRoleBinding".
$ kubectl -n kubernetes-dashboard delete serviceaccount admin-user
$ kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user

$ kubectl rollout restart deployment <deployment-name>
$ kubectl scale deployment <deployment-name> --replicas=0

Notes:
Port configurations for Kubernetes Services
In Kubernetes there are several different port configurations for Kubernetes services:
1) Port - exposes the Kubernetes service on the specified port within the cluster. Other pods within the cluster can communicate with this server on the specified port.
2) TargetPort - is the port on which the service will send requests to, that your pod will be listening on. Your application in the container will need to be listening on this port also.
3) NodePort - exposes a service externally to the cluster by means of the target nodes IP address and the NodePort. NodePort is the default setting if the port field is not specified.
