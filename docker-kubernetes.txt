Kubernetes for Docker

[[[Install kubectl on Linux]]]
A) Install kubectl binary with curl on Linux
Download the latest release with the command:
$ curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

Validate the binary (optional), download the kubectl checksum file:
$ curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"

Validate the kubectl binary against the checksum file:
$ echo "$(<kubectl.sha256) kubectl" | sha256sum --check

Install kubectl
$ sudo install -o root -g root -m 0755 kubectl /usr/bin/kubectl

B) Install using native package management
***Ubuntu, Debian or HypriotOS***
$ sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2 curl
$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
$ echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
$ sudo apt-get update
$ sudo apt-get install -y kubectl kubeadm kubelet
$ sudo apt-mark hold kubelet kubeadm kubectl   (*optional)

***CentOS, RHEL or Fedora***
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64       (***)
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-\$basearch   (***)
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
$ yum install -y kubectl kubeadm kubelet
$ sudo systemctl enable --now kubelet   (*optional)

[[[Optional kubectl configurations]]]
C) Install bash-completion
Bash-completion is provided by many package managers. You can install it with:
$ sudo apt-get install bash-completion
The above commands create "/usr/share/bash-completion/bash_completion", which is the main script of bash-completion. Depending on your package manager, you have to manually source this file in your "~/.bashrc" file.

To find out, reload your shell and run "type _init_completion". If the command succeeds, you're already set, otherwise add the following to your "~/.bashrc" file:
$ source /usr/share/bash-completion/bash_completion
Reload your shell and verify that bash-completion is correctly installed by typing "type _init_completion".

D) Enable kubectl autocompletion 
You now need to ensure that the kubectl completion script gets sourced in all your shell sessions. There are two ways in which you can do this:

- Source the completion script in your "~/.bashrc" file:
$ echo 'source <(kubectl completion bash)' >>~/.bashrc
- Add the completion script to the "/etc/bash_completion.d" directory:
$ kubectl completion bash >/etc/bash_completion.d/kubectl

If you have an alias for kubectl, you can extend shell completion to work with that alias:
$ echo 'alias k=kubectl' >>~/.bashrc
$ echo 'complete -F __start_kubectl k' >>~/.bashrc

[[[Configure cgroup driver used by kubelet on control-plane node]]]
E) Control groups are used to constrain resources that are allocated to processes
When 'systemd' is chosen as the init system for a Linux distribution, the init process generates and consumes a root control group (cgroup) and acts as a cgroup manager. Systemd has a tight integration with cgroups and allocates a cgroup per systemd unit. It's possible to configure your container runtime and the kubelet to use 'cgroupfs'. Using 'cgroupfs' alongside systemd means that there will be two different cgroup managers. A single cgroup manager simplifies the view of what resources are being allocated and will by default have a more consistent view of the available and in-use resources. When there are two cgroup managers on a system, you end up with two views of those resources. In the field, people have reported cases where nodes that are configured to use 'cgroupfs' for the kubelet and Docker, but 'systemd' for the rest of the processes, become unstable under resource pressure. Changing the settings such that your container runtime and kubelet use 'systemd' as the cgroup driver stabilized the system.

Your cgroup driver used by Kubelet must match Docker’s cgroup driver.

$ docker info | grep -i cgroup
you will get a Docker cgroup driver. And with the command

$ sudo cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
the Kubelet configuration. In the column with the flag “–cgroup-driver=” you can see the setting.
--> Environment="KUBELET_CGROUP_ARGS=--cgroup-driver=systemd"

When I installed Docker v.1.12 and kubeadm on CentOS 7, the Docker cgroup driver was specified with cgroupfs and for Kubelet with systemd. If you find the same setting, you can adjust the kubelet config with the following command:
$ sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
or
$ CG=$(sudo docker info 2>/dev/null | sed -n 's/Cgroup Driver: \(.*\)/\1/p')
$ sed -i "s/cgroup-driver=systemd/cgroup-driver=$CG/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

If an adjustment has been made, your Kubelet must be restarted.
$ sudo systemctl daemon-reload
$ sudo systemctl restart kubelet

[[[Running KIND Inside A Kubernetes Cluster]]]
F) KinD: Kubernetes in Docker like Docker in Docker, but for Kubernetes!
KinD or Kubernetes in Docker is a suite of tools for local Kubernetes “clusters” where each “node” is a Docker container. KinD was primarily designed for testing Kubernetes itself, but may be used for local development or CI. kind is divided into go packages implementing most of the functionality, a command line for users, and a “node” base image. The intent is that the kind the suite of packages should eventually be importable and reusable by other tools (e.g. kubetest) while the CLI provides a quick way to use and debug these packages.

$ curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.10.0/kind-linux-amd64
$ sudo install -o root -g root -m 0755 kind /usr/bin/kind
$ kind version

Once you have docker running you can create a cluster with:
$ kind create cluster

To delete your cluster use:
$ kind delete cluster

To create a cluster from Kubernetes source:
- ensure that Kubernetes is cloned in "$(go env GOPATH)/src/k8s.io/kubernetes"
- build a node image and create a cluster with:
$ kind build node-image
$ kind create cluster --image kindest/node:latest
